"""
éŸ³é¢‘ç”Ÿæˆç³»ç»Ÿ - éŸ³é¢‘å¤„ç†å™¨
åˆå¹¶ generate_audio.py å’Œ audio_util.py çš„ AudioProcessor
"""

from typing import List, Optional
import numpy as np
from scipy.signal import butter, filtfilt


class AudioProcessor:
    """ç»Ÿä¸€çš„éŸ³é¢‘å¤„ç†å·¥å…·é›†"""

    # ========================================================================
    # åŸºç¡€å¤„ç†
    # ========================================================================

    @staticmethod
    def normalize(audio: np.ndarray, target_peak: float = 0.9, volume: float = 1.0) -> np.ndarray:
        """
        å½’ä¸€åŒ–éŸ³é¢‘åˆ°ç›®æ ‡å³°å€¼å¹¶åº”ç”¨éŸ³é‡

        Args:
            audio: éŸ³é¢‘æ•°ç»„
            target_peak: ç›®æ ‡å³°å€¼ (0-1)
            volume: éŸ³é‡å€æ•° (0.0-2.0)ï¼Œ1.0 ä¸ºåŸå§‹éŸ³é‡

        Returns:
            å½’ä¸€åŒ–å¹¶è°ƒæ•´éŸ³é‡åçš„éŸ³é¢‘
        """
        max_val = np.max(np.abs(audio))
        if max_val > 0:
            return audio * (target_peak / max_val) * volume
        return audio

    @staticmethod
    def to_int16(audio: np.ndarray) -> np.ndarray:
        """è½¬æ¢ä¸º16ä½æ•´æ•°"""
        audio = np.clip(audio, -1.0, 1.0)
        return (audio * 32767).astype(np.int16)

    @staticmethod
    def to_float(audio: np.ndarray) -> np.ndarray:
        """è½¬æ¢ä¸ºæµ®ç‚¹æ•°"""
        if audio.dtype == np.int16:
            return audio.astype(np.float64) / 32767.0
        return audio.astype(np.float64)

    # ========================================================================
    # æ·¡å…¥æ·¡å‡º
    # ========================================================================

    @staticmethod
    def apply_fade(audio: np.ndarray, fade_in_samples: int = 0,
                   fade_out_samples: int = 0) -> np.ndarray:
        """
        åº”ç”¨æ·¡å…¥æ·¡å‡º

        Args:
            audio: éŸ³é¢‘æ•°ç»„
            fade_in_samples: æ·¡å…¥é‡‡æ ·æ•°
            fade_out_samples: æ·¡å‡ºé‡‡æ ·æ•°
        """
        result = audio.copy().astype(np.float64)

        if fade_in_samples > 0 and fade_in_samples < len(result):
            fade_in = np.linspace(0, 1, fade_in_samples)
            result[:fade_in_samples] *= fade_in

        if fade_out_samples > 0 and fade_out_samples < len(result):
            fade_out = np.linspace(1, 0, fade_out_samples)
            result[-fade_out_samples:] *= fade_out

        return result

    # ========================================================================
    # æ»¤æ³¢å™¨
    # ========================================================================

    @staticmethod
    def lowpass_filter(audio: np.ndarray, cutoff: float,
                       sample_rate: int, order: int = 4) -> np.ndarray:
        """ä½é€šæ»¤æ³¢å™¨"""
        nyquist = sample_rate / 2
        normalized_cutoff = min(cutoff / nyquist, 0.99)
        b, a = butter(order, normalized_cutoff, btype='low')
        return filtfilt(b, a, audio)

    @staticmethod
    def highpass_filter(audio: np.ndarray, cutoff: float,
                        sample_rate: int, order: int = 2) -> np.ndarray:
        """é«˜é€šæ»¤æ³¢å™¨ï¼ˆä» audio_util æ·»åŠ ï¼‰"""
        nyquist = sample_rate / 2
        normalized_cutoff = max(cutoff / nyquist, 0.001)
        normalized_cutoff = min(normalized_cutoff, 0.99)
        b, a = butter(order, normalized_cutoff, btype='high')
        return filtfilt(b, a, audio)

    @staticmethod
    def bandpass_filter(audio: np.ndarray, low_cutoff: float,
                        high_cutoff: float, sample_rate: int,
                        order: int = 2) -> np.ndarray:
        """å¸¦é€šæ»¤æ³¢å™¨ï¼ˆä» audio_util æ·»åŠ ï¼‰"""
        nyquist = sample_rate / 2
        low = max(low_cutoff / nyquist, 0.001)
        high = min(high_cutoff / nyquist, 0.99)
        b, a = butter(order, [low, high], btype='band')
        return filtfilt(b, a, audio)

    # ========================================================================
    # å‰Šæ³¢å’Œå¢ç›Š
    # ========================================================================

    @staticmethod
    def soft_clip(audio: np.ndarray, threshold: float = 0.8) -> np.ndarray:
        """
        è½¯å‰Šæ³¢
        ä½¿ç”¨ tanh å‡½æ•°å¹³æ»‘å‹ç¼©ï¼Œé˜²æ­¢ç¡¬å‰Šæ³¢å¤±çœŸ
        """
        result = audio.copy()
        mask = np.abs(result) > threshold
        if np.any(mask):
            result[mask] = threshold * np.sign(result[mask]) * np.tanh(
                np.abs(result[mask]) / threshold
            )
        return result

    @staticmethod
    def hard_clip(audio: np.ndarray, threshold: float = 1.0) -> np.ndarray:
        """ç¡¬å‰Šæ³¢"""
        return np.clip(audio, -threshold, threshold)

    @staticmethod
    def apply_gain(audio: np.ndarray, gain_db: float) -> np.ndarray:
        """åº”ç”¨å¢ç›Šï¼ˆåˆ†è´ï¼‰"""
        gain_linear = 10 ** (gain_db / 20)
        return audio * gain_linear

    @staticmethod
    def apply_volume(audio: np.ndarray, volume: float) -> np.ndarray:
        """
        åº”ç”¨éŸ³é‡è°ƒæ•´ï¼ˆçº¿æ€§ï¼‰

        Args:
            audio: éŸ³é¢‘æ•°ç»„
            volume: éŸ³é‡å€æ•° (0.0-2.0)ï¼Œ1.0 ä¸ºåŸå§‹éŸ³é‡

        Returns:
            è°ƒæ•´åçš„éŸ³é¢‘
        """
        return audio * volume

    # ========================================================================
    # æ™ºèƒ½æ··éŸ³ - ğŸ”¥ æ ¸å¿ƒåŠŸèƒ½
    # ========================================================================

    @staticmethod
    def mix(audios: List[np.ndarray],
            volumes: Optional[List[float]] = None,
            use_smart_mixing: bool = True) -> np.ndarray:
        """
        æ™ºèƒ½æ··åˆå¤šä¸ªéŸ³é¢‘ - åˆå¹¶ç‰ˆæœ¬

        èåˆä¼˜åŠ¿ï¼š
        - RMSå½’ä¸€åŒ–ï¼ˆgenerate_audio.py çš„ âˆšNè§„åˆ™ï¼‰
        - è½¯å‰Šæ³¢é˜²æ­¢å¤±çœŸ
        - æ”¯æŒè‡ªå®šä¹‰éŸ³é‡

        Args:
            audios: éŸ³é¢‘æ•°ç»„åˆ—è¡¨
            volumes: éŸ³é‡åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            use_smart_mixing: æ˜¯å¦ä½¿ç”¨æ™ºèƒ½æ··éŸ³ï¼ˆRMSå½’ä¸€åŒ– + è½¯å‰Šæ³¢ï¼‰

        Returns:
            æ··åˆåçš„éŸ³é¢‘
        """
        if not audios:
            return np.array([], dtype=np.float64)

        if len(audios) == 1:
            return audios[0]

        if volumes is None:
            volumes = [1.0] * len(audios)

        # å¯¹é½é•¿åº¦
        max_length = max(len(a) for a in audios)
        aligned_audios = []

        for audio, vol in zip(audios, volumes):
            if len(audio) < max_length:
                padded = np.pad(audio, (0, max_length - len(audio)), mode='constant')
                aligned_audios.append(padded.astype(np.float64) * vol)
            else:
                aligned_audios.append(audio[:max_length].astype(np.float64) * vol)

        # ç®€å•ç›¸åŠ 
        mixed = np.sum(aligned_audios, axis=0)

        if use_smart_mixing:
            # ğŸ”¥ å…³é”®ï¼šä½¿ç”¨RMSå½’ä¸€åŒ–ï¼ˆâˆšnè§„åˆ™ï¼‰
            # è¿™ç¡®ä¿Nä¸ªéŸ³ç¬¦æ··åˆæ—¶ï¼Œæ€»éŸ³é‡æŒ‰âˆšNå¢é•¿ï¼Œè€Œä¸æ˜¯çº¿æ€§å¢é•¿
            # è¿™æ˜¯ä¸“ä¸šéŸ³é¢‘è½¯ä»¶çš„æ ‡å‡†åšæ³•ï¼ˆæ¥è‡ª generate_audio.pyï¼‰
            num_notes = len(aligned_audios)
            mixed = mixed / np.sqrt(num_notes)

            # åº”ç”¨è½¯å‰Šæ³¢é˜²æ­¢ç¡¬å‰Šæ³¢å¤±çœŸ
            mixed = AudioProcessor.soft_clip(mixed, threshold=0.9)

        return mixed

    # ========================================================================
    # ç›¸ä½å¤„ç†ï¼ˆç”¨äºå’Œå¼¦ä¼˜åŒ–ï¼‰
    # ========================================================================

    @staticmethod
    def apply_phase_randomization(audio: np.ndarray,
                                   frequency: float,
                                   sample_rate: int = 44100) -> np.ndarray:
        """
        åº”ç”¨ç›¸ä½éšæœºåŒ–ï¼ˆä» generate_audio.pyï¼‰

        å¯¹äºé¢„å½•çš„éŸ³é¢‘ï¼Œé€šè¿‡è½»å¾®çš„æ—¶é—´åç§»æ¥é¿å…ç›¸ä½å¯¹é½é—®é¢˜
        è¿™æ˜¯é’¢ç´è½¯ä»¶å¤„ç†å¤šéŸ³å åŠ çš„å…³é”®æŠ€æœ¯ä¹‹ä¸€
        """
        # éšæœºç›¸ä½åç§»ï¼ˆ0-2Ï€ï¼‰
        phase_offset = np.random.uniform(0, 2 * np.pi)

        # é€šè¿‡FFTåº”ç”¨ç›¸ä½åç§»
        fft = np.fft.rfft(audio)
        freqs = np.fft.rfftfreq(len(audio), 1/sample_rate)

        # åªåœ¨åŸºé¢‘é™„è¿‘åº”ç”¨ç›¸ä½åç§»
        fundamental_idx = np.argmin(np.abs(freqs - frequency))
        window_size = max(10, len(fft) // 100)

        start_idx = max(0, fundamental_idx - window_size)
        end_idx = min(len(fft), fundamental_idx + window_size)

        # åº”ç”¨ç›¸ä½åç§»
        fft[start_idx:end_idx] *= np.exp(1j * phase_offset)

        # è½¬æ¢å›æ—¶åŸŸ
        result = np.fft.irfft(fft, len(audio))
        return result.astype(np.float64)

    # ========================================================================
    # å…¶ä»–å·¥å…·ï¼ˆä» audio_utilï¼‰
    # ========================================================================

    @staticmethod
    def concatenate(audios: List[np.ndarray], gap_ms: float = 0,
                    sample_rate: int = 44100) -> np.ndarray:
        """è¿æ¥å¤šä¸ªéŸ³é¢‘"""
        if not audios:
            return np.array([], dtype=np.float64)

        gap_samples = int(gap_ms / 1000 * sample_rate)
        gap = np.zeros(gap_samples) if gap_samples > 0 else np.array([])

        result = []
        for i, audio in enumerate(audios):
            result.append(audio)
            if i < len(audios) - 1 and len(gap) > 0:
                result.append(gap)

        return np.concatenate(result)

    @staticmethod
    def reverse(audio: np.ndarray) -> np.ndarray:
        """åè½¬éŸ³é¢‘"""
        return audio[::-1].copy()

    @staticmethod
    def stereo_pan(audio: np.ndarray, pan: float = 0.0) -> np.ndarray:
        """ç«‹ä½“å£°å¹³è¡¡ï¼ˆ-1=å·¦ï¼Œ0=ä¸­ï¼Œ1=å³ï¼‰"""
        left_gain = np.sqrt(0.5 * (1 - pan))
        right_gain = np.sqrt(0.5 * (1 + pan))
        return np.column_stack([audio * left_gain, audio * right_gain])
