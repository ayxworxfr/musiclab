#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
éŸ³é¢‘æ–‡ä»¶ç”Ÿæˆè„šæœ¬ v4.0 (å¢å¼ºä¼˜åŒ–ç‰ˆ)
- æ·»åŠ éŸ³é¢‘è´¨é‡è¯„ä¼°ç³»ç»Ÿ
- å¢å¼ºé’¢ç´éŸ³è‰²çœŸå®åº¦ï¼ˆéŸ³æ¿å…±é¸£ã€ç´å¼¦è€¦åˆï¼‰
- æ”¯æŒå¤šè¿›ç¨‹å¹¶è¡Œç”Ÿæˆ
- æ·»åŠ é¢‘è°±åˆ†æå¯è§†åŒ–

ä½¿ç”¨æ–¹æ³•ï¼š
  python3 scripts/generate_audio.py
  python3 scripts/generate_audio.py --parallel         # å¹¶è¡Œæ¨¡å¼
  python3 scripts/generate_audio.py --analyze          # ç”Ÿæˆåˆ†æå›¾è¡¨
  python3 scripts/generate_audio.py --parallel --analyze  # å…¨åŠŸèƒ½

ä¾èµ–ï¼š
  pip3 install numpy scipy matplotlib librosa
"""

import io
import random
import subprocess
import sys
import warnings
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
from functools import partial
from multiprocessing import Pool, cpu_count
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import matplotlib
import numpy as np

matplotlib.use('Agg')  # éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt
from scipy import signal as scipy_signal
from scipy.io import wavfile
from scipy.ndimage import uniform_filter1d
from scipy.signal import butter, filtfilt

# è®¾ç½®æ ‡å‡†è¾“å‡ºä¸ºUTF-8ç¼–ç ï¼ˆWindowså…¼å®¹ï¼‰
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# å¿½ç•¥ matplotlib è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning)


# ============================================================================
# å¸¸é‡å®šä¹‰
# ============================================================================

# MIDIéŸ³ç¬¦èŒƒå›´
MIDI_MIN = 21  # A0
MIDI_MAX = 108  # C8
MIDI_RANGE = range(MIDI_MIN, MIDI_MAX + 1)

# åˆ†ææ ·æœ¬éŸ³ç¬¦ï¼ˆè¦†ç›–ä¸åŒéŸ³åŸŸï¼‰
SAMPLE_NOTES_FOR_ANALYSIS = [21, 36, 48, 60, 72, 84, 96, 108]

# è´¨é‡é˜ˆå€¼
PITCH_ERROR_THRESHOLD_CENTS = 10  # éŸ³é«˜è¯¯å·®é˜ˆå€¼ï¼ˆéŸ³åˆ†ï¼‰
SNR_THRESHOLD_DB = 15  # ä¿¡å™ªæ¯”é˜ˆå€¼ï¼ˆåˆ†è´ï¼‰- å¯¹åˆæˆéŸ³é¢‘ä½¿ç”¨æ›´åˆç†çš„é˜ˆå€¼
THD_THRESHOLD_PERCENT = 5  # æ€»è°æ³¢å¤±çœŸé˜ˆå€¼ï¼ˆç™¾åˆ†æ¯”ï¼‰

# æ ‡å‡†éŸ³é«˜
A4_FREQUENCY = 440.0
A4_MIDI = 69


# ============================================================================
# é…ç½®æ¨¡å‹
# ============================================================================

@dataclass
class AudioConfig:
    """éŸ³é¢‘åŸºç¡€é…ç½®"""
    sample_rate: int = 44100
    bit_depth: int = 16
    channels: int = 1
    
    @property
    def max_amplitude(self) -> int:
        """æœ€å¤§æŒ¯å¹…å€¼"""
        return 2 ** (self.bit_depth - 1) - 1


@dataclass
class EnvelopeConfig:
    """ADSR åŒ…ç»œé…ç½®"""
    attack: float = 0.005    # èµ·éŸ³æ—¶é—´ï¼ˆç§’ï¼‰
    decay: float = 0.1       # è¡°å‡æ—¶é—´ï¼ˆç§’ï¼‰
    sustain: float = 0.6     # æŒç»­ç”µå¹³ (0-1)
    release: float = 0.8     # é‡Šæ”¾æ—¶é—´ï¼ˆç§’ï¼‰


@dataclass
class HarmonicConfig:
    """æ³›éŸ³é…ç½®"""
    harmonic_number: int     # æ³›éŸ³ç¼–å·ï¼ˆ1=åŸºé¢‘ï¼‰
    amplitude: float         # æŒ¯å¹… (0-1)
    decay_rate: float        # è¡°å‡é€Ÿåº¦


@dataclass
class EnhancedPianoConfig:
    """å¢å¼ºçš„é’¢ç´é…ç½®"""
    duration: float = 2.5
    envelope: EnvelopeConfig = field(default_factory=EnvelopeConfig)
    
    # æ›´ä¸°å¯Œçš„æ³›éŸ³ç»“æ„ï¼ˆåŸºäºçœŸå®é’¢ç´åˆ†æï¼‰
    harmonics: List[HarmonicConfig] = field(default_factory=lambda: [
        HarmonicConfig(1, 1.0, 2.0),     # åŸºé¢‘
        HarmonicConfig(2, 0.6, 2.5),     # äºŒæ¬¡æ³›éŸ³
        HarmonicConfig(3, 0.35, 3.0),     # ä¸‰æ¬¡æ³›éŸ³
        HarmonicConfig(4, 0.2, 3.5),
        HarmonicConfig(5, 0.12, 4.0),
        HarmonicConfig(6, 0.08, 4.5),
        HarmonicConfig(7, 0.05, 5.0),
        HarmonicConfig(8, 0.03, 5.5),
        HarmonicConfig(9, 0.02, 6.0),
        HarmonicConfig(10, 0.01, 6.5),
    ])
    
    # å¤±è°å‚æ•°
    inharmonic_detune: float = 1.003
    inharmonic_amplitude: float = 0.02
    
    # éŸ³æ¿å…±é¸£ï¼ˆä½é¢‘å…±æŒ¯ï¼‰
    soundboard_resonance: float = 0.05
    soundboard_freq_offset: float = 0.5  # åŠéŸ³
    
    # ç´å¼¦è€¦åˆï¼ˆç›¸é‚»å¼¦çš„å…±æŒ¯ï¼‰
    string_coupling: float = 0.03
    
    # æ·¡å…¥æ·¡å‡º
    fade_in: float = 0.002
    fade_out: float = 0.05


@dataclass
class MetronomeConfig:
    """èŠ‚æ‹å™¨é…ç½®"""
    duration: float = 0.08
    strong_beat_freq: float = 440
    weak_beat_freq: float = 660
    decay_rate: float = 40
    lowpass_cutoff: float = 3000


class EffectType(Enum):
    """æ•ˆæœéŸ³ç±»å‹"""
    CORRECT = "correct"
    WRONG = "wrong"
    COMPLETE = "complete"
    LEVEL_UP = "levelUp"


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def midi_to_frequency(midi: int) -> float:
    """MIDIéŸ³ç¬¦è½¬é¢‘ç‡"""
    return A4_FREQUENCY * (2.0 ** ((midi - A4_MIDI) / 12.0))


def midi_to_note_name(midi: int) -> str:
    """MIDIè½¬éŸ³ç¬¦åç§°"""
    notes = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
    octave = (midi // 12) - 1
    note = notes[midi % 12]
    return f"{note}{octave}"


# ============================================================================
# éŸ³é¢‘å¤„ç†å·¥å…·
# ============================================================================

class AudioProcessor:
    """éŸ³é¢‘å¤„ç†å·¥å…·ç±»"""
    
    @staticmethod
    def apply_fade(audio: np.ndarray, fade_in_samples: int, fade_out_samples: int) -> np.ndarray:
        """åº”ç”¨æ·¡å…¥æ·¡å‡º"""
        result = audio.copy().astype(np.float64)
        
        if fade_in_samples > 0:
            fade_in = np.linspace(0, 1, fade_in_samples)
            result[:fade_in_samples] *= fade_in
        
        if fade_out_samples > 0:
            fade_out = np.linspace(1, 0, fade_out_samples)
            result[-fade_out_samples:] *= fade_out
        
        return result
    
    @staticmethod
    def lowpass_filter(audio: np.ndarray, cutoff: float, sample_rate: int) -> np.ndarray:
        """ä½é€šæ»¤æ³¢å™¨"""
        nyquist = sample_rate / 2
        normalized_cutoff = min(cutoff / nyquist, 0.99)
        b, a = butter(4, normalized_cutoff, btype='low')
        return filtfilt(b, a, audio)
    
    @staticmethod
    def normalize(audio: np.ndarray, target_level: float = 0.9) -> np.ndarray:
        """å½’ä¸€åŒ–éŸ³é¢‘"""
        max_val = np.max(np.abs(audio))
        if max_val > 0:
            audio = audio / max_val * target_level
        return audio
    
    @staticmethod
    def to_int16(audio: np.ndarray) -> np.ndarray:
        """è½¬æ¢ä¸º16ä½æ•´æ•°"""
        return (audio * 32767).astype(np.int16)
    
    @staticmethod
    def mix(audios: List[np.ndarray], 
            volumes: Optional[List[float]] = None,
            use_smart_mixing: bool = True) -> np.ndarray:
        """
        æ™ºèƒ½æ··åˆå¤šä¸ªéŸ³é¢‘
        
        ä½¿ç”¨RMSå½’ä¸€åŒ–è€Œä¸æ˜¯å³°å€¼å½’ä¸€åŒ–ï¼Œé¿å…å¤šéŸ³å åŠ æ—¶çš„å‰Šæ³¢é—®é¢˜
        è¿™æ˜¯é’¢ç´è½¯ä»¶èƒ½å¤ŸåŒæ—¶æ’­æ”¾å¤šä¸ªéŸ³ç¬¦çš„å…³é”®æŠ€æœ¯
        
        Args:
            audios: éŸ³é¢‘æ•°ç»„åˆ—è¡¨
            volumes: éŸ³é‡åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            use_smart_mixing: æ˜¯å¦ä½¿ç”¨æ™ºèƒ½æ··éŸ³ï¼ˆRMSå½’ä¸€åŒ– + è½¯å‰Šæ³¢ï¼‰
        
        Returns:
            æ··åˆåçš„éŸ³é¢‘
        """
        if not audios:
            return np.array([], dtype=np.float64)
        
        if len(audios) == 1:
            return audios[0]
        
        if volumes is None:
            volumes = [1.0] * len(audios)
        
        # å¯¹é½é•¿åº¦
        max_length = max(len(a) for a in audios)
        aligned_audios = []
        
        for audio, vol in zip(audios, volumes):
            if len(audio) < max_length:
                padded = np.pad(audio, (0, max_length - len(audio)), mode='constant')
                aligned_audios.append(padded.astype(np.float64) * vol)
            else:
                aligned_audios.append(audio[:max_length].astype(np.float64) * vol)
        
        # ç®€å•ç›¸åŠ 
        mixed = np.sum(aligned_audios, axis=0)
        
        if use_smart_mixing:
            # å…³é”®ï¼šä½¿ç”¨RMSå½’ä¸€åŒ–ï¼ˆâˆšnè§„åˆ™ï¼‰
            # è¿™ç¡®ä¿Nä¸ªéŸ³ç¬¦æ··åˆæ—¶ï¼Œæ€»éŸ³é‡ä¸ä¼šçº¿æ€§å¢é•¿ï¼Œè€Œæ˜¯æŒ‰âˆšNå¢é•¿
            # è¿™æ˜¯ä¸“ä¸šéŸ³é¢‘è½¯ä»¶çš„æ ‡å‡†åšæ³•
            num_notes = len(aligned_audios)
            mixed = mixed / np.sqrt(num_notes)
            
            # åº”ç”¨è½¯å‰Šæ³¢é˜²æ­¢ç¡¬å‰Šæ³¢å¤±çœŸ
            mixed = AudioProcessor.soft_clip(mixed, threshold=0.9)
        
        return mixed
    
    @staticmethod
    def soft_clip(audio: np.ndarray, threshold: float = 0.9) -> np.ndarray:
        """
        è½¯å‰Šæ³¢ - é˜²æ­¢ç¡¬å‰Šæ³¢å¤±çœŸ
        
        å½“éŸ³é¢‘è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œä½¿ç”¨tanhå‡½æ•°å¹³æ»‘å‹ç¼©ï¼Œè€Œä¸æ˜¯ç¡¬æˆªæ–­
        """
        result = audio.copy()
        mask = np.abs(result) > threshold
        if np.any(mask):
            result[mask] = threshold * np.sign(result[mask]) * np.tanh(
                np.abs(result[mask]) / threshold
            )
        return result
    
    @staticmethod
    def apply_phase_randomization(audio: np.ndarray, 
                                   frequency: float,
                                   sample_rate: int = 44100) -> np.ndarray:
        """
        åº”ç”¨ç›¸ä½éšæœºåŒ–
        
        å¯¹äºé¢„å½•çš„éŸ³é¢‘ï¼Œé€šè¿‡è½»å¾®çš„æ—¶é—´åç§»æ¥é¿å…ç›¸ä½å¯¹é½é—®é¢˜
        è¿™æ˜¯é’¢ç´è½¯ä»¶å¤„ç†å¤šéŸ³å åŠ çš„å…³é”®æŠ€æœ¯ä¹‹ä¸€
        """
        # éšæœºç›¸ä½åç§»ï¼ˆ0-2Ï€ï¼‰
        phase_offset = np.random.uniform(0, 2 * np.pi)
        
        # é€šè¿‡FFTåº”ç”¨ç›¸ä½åç§»
        fft = np.fft.rfft(audio)
        freqs = np.fft.rfftfreq(len(audio), 1/sample_rate)
        
        # åªåœ¨åŸºé¢‘é™„è¿‘åº”ç”¨ç›¸ä½åç§»
        fundamental_idx = np.argmin(np.abs(freqs - frequency))
        window_size = max(10, len(fft) // 100)  # å½±å“èŒƒå›´
        
        start_idx = max(0, fundamental_idx - window_size)
        end_idx = min(len(fft), fundamental_idx + window_size)
        
        # åº”ç”¨ç›¸ä½åç§»
        fft[start_idx:end_idx] *= np.exp(1j * phase_offset)
        
        # è½¬æ¢å›æ—¶åŸŸ
        result = np.fft.irfft(fft, len(audio))
        return result.astype(np.float64)


class EnvelopeGenerator:
    """åŒ…ç»œç”Ÿæˆå™¨"""
    
    @staticmethod
    def adsr(num_samples: int, sample_rate: int, config: EnvelopeConfig) -> np.ndarray:
        """ç”ŸæˆADSRåŒ…ç»œ"""
        attack_samples = int(config.attack * sample_rate)
        decay_samples = int(config.decay * sample_rate)
        release_samples = int(config.release * sample_rate)
        sustain_samples = max(0, num_samples - attack_samples - decay_samples - release_samples)
        
        if sustain_samples <= 0:
            sustain_samples = 0
            release_samples = num_samples - attack_samples - decay_samples
        
        envelope = np.zeros(num_samples)
        pos = 0
        
        # Attack
        if attack_samples > 0:
            envelope[pos:pos + attack_samples] = np.linspace(0, 1, attack_samples)
            pos += attack_samples
        
        # Decay
        if decay_samples > 0 and pos + decay_samples <= num_samples:
            envelope[pos:pos + decay_samples] = np.linspace(1, config.sustain, decay_samples)
            pos += decay_samples
        
        # Sustain (with slow decay)
        if sustain_samples > 0 and pos + sustain_samples <= num_samples:
            envelope[pos:pos + sustain_samples] = config.sustain * np.exp(
                -0.5 * np.linspace(0, 1, sustain_samples)
            )
            pos += sustain_samples
        
        # Release
        if release_samples > 0 and pos < num_samples:
            remaining = num_samples - pos
            actual_release = min(remaining, release_samples)
            start_level = envelope[pos - 1] if pos > 0 else config.sustain
            envelope[pos:pos + actual_release] = start_level * np.exp(
                -3 * np.linspace(0, 1, actual_release)
            )
        
        return envelope
    
    @staticmethod
    def percussive(num_samples: int, sample_rate: int, attack_ms: float = 1, decay_rate: float = 40) -> np.ndarray:
        """æ‰“å‡»ä¹åŒ…ç»œ"""
        attack = int(attack_ms * sample_rate / 1000)
        decay = num_samples - attack
        
        envelope = np.zeros(num_samples)
        envelope[:attack] = np.linspace(0, 1, attack)
        envelope[attack:] = np.exp(-decay_rate * np.linspace(0, 1, decay))
        
        return envelope


# ============================================================================
# éŸ³é¢‘è´¨é‡åˆ†æå™¨
# ============================================================================

class AudioQualityAnalyzer:
    """éŸ³é¢‘è´¨é‡åˆ†æå™¨"""
    
    @staticmethod
    def analyze_spectrum(audio: np.ndarray, sr: int, midi_note: int) -> Dict:
        """åˆ†æé¢‘è°±ï¼ŒéªŒè¯éŸ³é«˜å‡†ç¡®æ€§"""
        # FFTåˆ†æ
        fft = np.fft.rfft(audio)
        freqs = np.fft.rfftfreq(len(audio), 1/sr)
        magnitude = np.abs(fft)
        
        # æ‰¾åˆ°ä¸»é¢‘ç‡ï¼ˆå¿½ç•¥ç›´æµåˆ†é‡ï¼‰
        magnitude[0] = 0
        peak_idx = np.argmax(magnitude)
        detected_freq = freqs[peak_idx]
        
        # æœŸæœ›é¢‘ç‡
        expected_freq = midi_to_frequency(midi_note)
        
        # è®¡ç®—è¯¯å·®ï¼ˆéŸ³åˆ†ï¼‰
        if detected_freq > 0:
            error_cents = 1200 * np.log2(detected_freq / expected_freq)
        else:
            error_cents = 999
        
        return {
            'detected_freq': detected_freq,
            'expected_freq': expected_freq,
            'error_cents': error_cents,
            'is_accurate': abs(error_cents) < PITCH_ERROR_THRESHOLD_CENTS
        }
    
    @staticmethod
    def calculate_snr(audio: np.ndarray, sr: int = 44100) -> float:
        """è®¡ç®—ä¿¡å™ªæ¯”ï¼ˆSignal-to-Noise Ratioï¼‰
        
        æ”¹è¿›çš„è®¡ç®—æ–¹æ³•ï¼š
        - ä½¿ç”¨é¢‘è°±åˆ†æï¼Œè®¡ç®—åŸºé¢‘åŠå…¶è°æ³¢çš„èƒ½é‡ï¼ˆä¿¡å·ï¼‰
        - è®¡ç®—éè°æ³¢é¢‘ç‡çš„èƒ½é‡ï¼ˆå™ªå£°ï¼‰
        - å¯¹äºåˆæˆéŸ³é¢‘ï¼Œè¿™ç§æ–¹æ³•æ›´å‡†ç¡®
        """
        # ä½¿ç”¨FFTè¿›è¡Œé¢‘è°±åˆ†æ
        fft = np.fft.rfft(audio)
        freqs = np.fft.rfftfreq(len(audio), 1/sr)
        magnitude = np.abs(fft)
        
        # æ‰¾åˆ°ä¸»å³°ï¼ˆåŸºé¢‘ï¼‰
        magnitude_copy = magnitude.copy()
        magnitude_copy[0] = 0  # å¿½ç•¥ç›´æµåˆ†é‡
        peak_idx = np.argmax(magnitude_copy)
        peak_freq = freqs[peak_idx]
        
        # è®¡ç®—ä¿¡å·åŠŸç‡ï¼ˆåŸºé¢‘åŠå…¶å‰8ä¸ªè°æ³¢çš„èƒ½é‡ï¼‰
        signal_power = 0
        for n in range(1, 9):
            harmonic_freq = peak_freq * n
            if harmonic_freq < freqs[-1]:
                # æ‰¾åˆ°æœ€æ¥è¿‘çš„é¢‘ç‚¹
                idx = np.argmin(np.abs(freqs - harmonic_freq))
                signal_power += magnitude[idx] ** 2
        
        # è®¡ç®—å™ªå£°åŠŸç‡ï¼ˆæ€»åŠŸç‡å‡å»ä¿¡å·åŠŸç‡ï¼‰
        total_power = np.sum(magnitude ** 2)
        noise_power = total_power - signal_power
        
        # å¦‚æœå™ªå£°åŠŸç‡å¤ªå°ï¼Œè¯´æ˜ä¿¡å·è´¨é‡å¾ˆå¥½
        if noise_power < 1e-10 or signal_power < 1e-10:
            return 100.0
        
        # è®¡ç®—SNRï¼ˆåˆ†è´ï¼‰
        snr = 10 * np.log10(signal_power / noise_power)
        return snr
    
    @staticmethod
    def calculate_thd(audio: np.ndarray, sr: int, fundamental_freq: float) -> float:
        """è®¡ç®—æ€»è°æ³¢å¤±çœŸï¼ˆTotal Harmonic Distortionï¼‰"""
        fft = np.fft.rfft(audio)
        freqs = np.fft.rfftfreq(len(audio), 1/sr)
        magnitude = np.abs(fft)
        
        # åŸºé¢‘èƒ½é‡
        fundamental_idx = np.argmin(np.abs(freqs - fundamental_freq))
        fundamental_power = magnitude[fundamental_idx] ** 2
        
        # è°æ³¢èƒ½é‡ï¼ˆ2-8æ¬¡è°æ³¢ï¼‰
        harmonic_power = 0
        for n in range(2, 9):
            harmonic_freq = fundamental_freq * n
            if harmonic_freq < sr / 2:
                harmonic_idx = np.argmin(np.abs(freqs - harmonic_freq))
                harmonic_power += magnitude[harmonic_idx] ** 2
        
        if fundamental_power > 0:
            thd = np.sqrt(harmonic_power / fundamental_power) * 100
        else:
            thd = 0
        
        return thd


# ============================================================================
# éŸ³é¢‘å¯è§†åŒ–å·¥å…·
# ============================================================================

class AudioVisualizer:
    """éŸ³é¢‘å¯è§†åŒ–å·¥å…·"""
    
    @staticmethod
    def plot_analysis(audio: np.ndarray, sr: int, midi: int, output_path: Path):
        """ç”Ÿæˆå®Œæ•´çš„éŸ³é¢‘åˆ†æå›¾è¡¨"""
        fig = plt.figure(figsize=(14, 10))
        gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)
        
        # 1. æ³¢å½¢å›¾
        ax1 = fig.add_subplot(gs[0, :])
        t = np.linspace(0, len(audio)/sr, len(audio))
        ax1.plot(t, audio, linewidth=0.5, color='#2E86AB')
        ax1.set_title(f'Waveform - MIDI {midi} ({midi_to_note_name(midi)})', 
                      fontsize=12, fontweight='bold')
        ax1.set_xlabel('Time (s)')
        ax1.set_ylabel('Amplitude')
        ax1.grid(True, alpha=0.3)
        ax1.set_xlim(0, min(1.0, len(audio)/sr))  # åªæ˜¾ç¤ºå‰1ç§’
        
        # 2. é¢‘è°±å›¾ï¼ˆFFTï¼‰
        ax2 = fig.add_subplot(gs[1, 0])
        fft = np.fft.rfft(audio)
        freqs = np.fft.rfftfreq(len(audio), 1/sr)
        magnitude_db = 20 * np.log10(np.abs(fft) + 1e-10)
        
        ax2.plot(freqs, magnitude_db, linewidth=0.8, color='#A23B72')
        ax2.set_title('Frequency Spectrum', fontsize=11, fontweight='bold')
        ax2.set_xlabel('Frequency (Hz)')
        ax2.set_ylabel('Magnitude (dB)')
        ax2.set_xlim(0, 5000)
        ax2.grid(True, alpha=0.3)
        
        # æ ‡è®°ç†è®ºæ³›éŸ³ä½ç½®
        fundamental = midi_to_frequency(midi)
        for n in range(1, 9):
            harmonic_freq = fundamental * n
            if harmonic_freq < 5000:
                ax2.axvline(harmonic_freq, color='#F18F01', alpha=0.5, 
                           linestyle='--', linewidth=1)
                if n == 1:
                    ax2.text(harmonic_freq, ax2.get_ylim()[1]*0.95, 'Fâ‚€', 
                            ha='center', fontsize=8)
        
        # 3. å£°è°±å›¾ï¼ˆSpectrogramï¼‰
        ax3 = fig.add_subplot(gs[1, 1])
        f, t_spec, Sxx = scipy_signal.spectrogram(audio, sr, nperseg=1024)
        im = ax3.pcolormesh(t_spec, f, 10 * np.log10(Sxx + 1e-10), 
                            shading='gouraud', cmap='viridis')
        ax3.set_title('Spectrogram', fontsize=11, fontweight='bold')
        ax3.set_ylabel('Frequency (Hz)')
        ax3.set_xlabel('Time (s)')
        ax3.set_ylim(0, 5000)
        plt.colorbar(im, ax=ax3, label='dB')
        
        # 4. åŒ…ç»œå›¾
        ax4 = fig.add_subplot(gs[2, 0])
        envelope = np.abs(audio)
        smooth_envelope = uniform_filter1d(envelope, size=int(sr*0.01))
        ax4.plot(t, smooth_envelope, linewidth=1.5, color='#C73E1D')
        ax4.fill_between(t, smooth_envelope, alpha=0.3, color='#C73E1D')
        ax4.set_title('Envelope', fontsize=11, fontweight='bold')
        ax4.set_xlabel('Time (s)')
        ax4.set_ylabel('Amplitude')
        ax4.grid(True, alpha=0.3)
        ax4.set_xlim(0, len(audio)/sr)
        
        # 5. è´¨é‡æŒ‡æ ‡
        ax5 = fig.add_subplot(gs[2, 1])
        ax5.axis('off')
        
        analyzer = AudioQualityAnalyzer()
        spectrum_result = analyzer.analyze_spectrum(audio, sr, midi)
        snr = analyzer.calculate_snr(audio, sr)
        thd = analyzer.calculate_thd(audio, sr, fundamental)
        
        info_text = f"""
Quality Metrics:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
MIDI Note: {midi} ({midi_to_note_name(midi)})
Fundamental: {fundamental:.2f} Hz
Detected: {spectrum_result['detected_freq']:.2f} Hz
Pitch Error: {spectrum_result['error_cents']:.2f} cents
Accuracy: {'âœ“ PASS' if spectrum_result['is_accurate'] else 'âœ— FAIL'}

SNR: {snr:.1f} dB {'âœ“' if snr > SNR_THRESHOLD_DB else 'âœ—'}
THD: {thd:.2f}% {'âœ“' if thd < THD_THRESHOLD_PERCENT else 'âœ—'}

Duration: {len(audio)/sr:.2f} s
Sample Rate: {sr} Hz
        """
        
        ax5.text(0.1, 0.5, info_text, fontsize=10, family='monospace',
                verticalalignment='center')
        
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()


# ============================================================================
# éŸ³é¢‘ç”Ÿæˆå™¨åŸºç±»
# ============================================================================

class AudioGenerator(ABC):
    """éŸ³é¢‘ç”Ÿæˆå™¨æŠ½è±¡åŸºç±»"""
    
    def __init__(self, config: AudioConfig):
        self.config = config
        self.processor = AudioProcessor()
    
    @abstractmethod
    def generate(self) -> Tuple[np.ndarray, int]:
        """ç”ŸæˆéŸ³é¢‘æ•°æ®"""
        pass
    
    def _midi_to_frequency(self, midi: int) -> float:
        """MIDIéŸ³ç¬¦è½¬é¢‘ç‡"""
        return midi_to_frequency(midi)
    
    def _create_time_array(self, duration: float) -> np.ndarray:
        """åˆ›å»ºæ—¶é—´æ•°ç»„"""
        num_samples = int(self.config.sample_rate * duration)
        return np.linspace(0, duration, num_samples)


# ============================================================================
# å¢å¼ºé’¢ç´ç”Ÿæˆå™¨
# ============================================================================

class EnhancedPianoGenerator(AudioGenerator):
    """å¢å¼ºçš„é’¢ç´éŸ³è‰²ç”Ÿæˆå™¨ï¼ˆç‰©ç†å»ºæ¨¡ï¼‰"""
    
    def __init__(self, config: AudioConfig, piano_config: EnhancedPianoConfig):
        super().__init__(config)
        self.piano_config = piano_config
    
    def generate(self, midi_number: int) -> Tuple[np.ndarray, int]:
        """ç”Ÿæˆé’¢ç´éŸ³ç¬¦"""
        frequency = self._midi_to_frequency(midi_number)
        t = self._create_time_array(self.piano_config.duration)
        num_samples = len(t)
        
        # ç”Ÿæˆæ³›éŸ³å åŠ 
        audio = self._generate_harmonics(t, frequency)
        
        # æ·»åŠ è½»å¾®å¤±è°
        audio += self._generate_inharmonic(t, frequency)
        
        # æ·»åŠ éŸ³æ¿å…±é¸£
        audio += self._add_soundboard_resonance(t, frequency)
        
        # æ·»åŠ ç´å¼¦è€¦åˆ
        audio += self._add_string_coupling(t, frequency)
        
        # æ ¹æ®éŸ³é«˜è°ƒæ•´åŒ…ç»œ
        envelope_config = self._adjust_envelope_for_pitch(midi_number)
        envelope = EnvelopeGenerator.adsr(num_samples, self.config.sample_rate, envelope_config)
        audio *= envelope
        
        # åŠ¨æ€ä½é€šæ»¤æ³¢
        cutoff = self._calculate_dynamic_cutoff(midi_number, frequency)
        audio = self.processor.lowpass_filter(audio, cutoff, self.config.sample_rate)
        
        # æ·¡å…¥æ·¡å‡ºæ¶ˆé™¤æ‚éŸ³
        fade_in_samples = int(self.piano_config.fade_in * self.config.sample_rate)
        fade_out_samples = int(self.piano_config.fade_out * self.config.sample_rate)
        audio = self.processor.apply_fade(audio, fade_in_samples, fade_out_samples)
        
        # å½’ä¸€åŒ–å¹¶è½¬æ¢
        audio = self.processor.normalize(audio, 0.9)
        audio = self.processor.to_int16(audio)
        
        return audio, self.config.sample_rate
    
    def _generate_harmonics(self, t: np.ndarray, base_freq: float) -> np.ndarray:
        """ç”Ÿæˆæ³›éŸ³"""
        audio = np.zeros_like(t)
        
        for harmonic in self.piano_config.harmonics:
            freq = base_freq * harmonic.harmonic_number
            if freq > self.config.sample_rate / 2:
                continue
            
            envelope = np.exp(-harmonic.decay_rate * t)
            wave = harmonic.amplitude * np.sin(2 * np.pi * freq * t) * envelope
            audio += wave
        
        return audio
    
    def _generate_inharmonic(self, t: np.ndarray, base_freq: float) -> np.ndarray:
        """ç”Ÿæˆè½»å¾®å¤±è°æˆåˆ†"""
        detuned_freq = base_freq * self.piano_config.inharmonic_detune
        return (self.piano_config.inharmonic_amplitude * 
                np.sin(2 * np.pi * detuned_freq * t) * 
                np.exp(-3 * t))
    
    def _add_soundboard_resonance(self, t: np.ndarray, base_freq: float) -> np.ndarray:
        """æ·»åŠ éŸ³æ¿å…±é¸£"""
        resonance_freq = base_freq * (2 ** (-self.piano_config.soundboard_freq_offset / 12))
        resonance = (self.piano_config.soundboard_resonance * 
                     np.sin(2 * np.pi * resonance_freq * t) * 
                     np.exp(-1.5 * t))
        return resonance
    
    def _add_string_coupling(self, t: np.ndarray, base_freq: float) -> np.ndarray:
        """æ·»åŠ ç´å¼¦è€¦åˆæ•ˆæœ"""
        coupling = np.zeros_like(t)
        for semitone in [-1, 1]:
            coupled_freq = base_freq * (2 ** (semitone / 12))
            coupling += (self.piano_config.string_coupling * 
                         np.sin(2 * np.pi * coupled_freq * t) * 
                         np.exp(-4 * t))
        return coupling
    
    def _adjust_envelope_for_pitch(self, midi: int) -> EnvelopeConfig:
        """æ ¹æ®éŸ³é«˜è°ƒæ•´åŒ…ç»œï¼ˆé«˜éŸ³è¡°å‡æ›´å¿«ï¼‰"""
        config = EnvelopeConfig()
        
        if midi > 84:  # C6ä»¥ä¸Š
            config.decay = 0.05
            config.release = 0.4
        elif midi > 72:  # C5-C6
            config.decay = 0.08
            config.release = 0.6
        elif midi < 48:  # C3ä»¥ä¸‹
            config.decay = 0.15
            config.release = 1.2
        
        return config
    
    def _calculate_dynamic_cutoff(self, midi: int, frequency: float) -> float:
        """åŠ¨æ€è®¡ç®—ä½é€šæ»¤æ³¢æˆªæ­¢é¢‘ç‡"""
        if midi > 84:
            return min(12000, frequency * 4)
        elif midi > 72:
            return min(10000, frequency * 3)
        elif midi > 60:
            return min(8000, frequency * 2.5)
        else:
            return min(6000, frequency * 2)


# ============================================================================
# èŠ‚æ‹å™¨ç”Ÿæˆå™¨
# ============================================================================

class MetronomeGenerator(AudioGenerator):
    """èŠ‚æ‹å™¨éŸ³æ•ˆç”Ÿæˆå™¨"""
    
    def __init__(self, config: AudioConfig, metronome_config: MetronomeConfig):
        super().__init__(config)
        self.metronome_config = metronome_config
    
    def generate(self, is_strong: bool = False) -> Tuple[np.ndarray, int]:
        """ç”ŸæˆèŠ‚æ‹å™¨æ•²å‡»å£°"""
        t = self._create_time_array(self.metronome_config.duration)
        num_samples = len(t)
        
        base_freq = (self.metronome_config.strong_beat_freq if is_strong 
                     else self.metronome_config.weak_beat_freq)
        
        if is_strong:
            audio = self._generate_strong_beat(t, base_freq)
        else:
            audio = self._generate_weak_beat(t, base_freq)
        
        envelope = EnvelopeGenerator.percussive(
            num_samples, 
            self.config.sample_rate,
            attack_ms=1,
            decay_rate=self.metronome_config.decay_rate
        )
        audio *= envelope
        
        audio = self.processor.lowpass_filter(
            audio, 
            self.metronome_config.lowpass_cutoff, 
            self.config.sample_rate
        )
        
        fade_out_samples = int(0.01 * self.config.sample_rate)
        audio = self.processor.apply_fade(audio, 0, fade_out_samples)
        
        audio = self.processor.normalize(audio, 0.7)
        audio = self.processor.to_int16(audio)
        
        return audio, self.config.sample_rate
    
    def _generate_strong_beat(self, t: np.ndarray, base_freq: float) -> np.ndarray:
        """å¼ºæ‹"""
        return (0.5 * np.sin(2 * np.pi * base_freq * t) +
                0.3 * np.sin(2 * np.pi * base_freq * 2 * t) +
                0.15 * np.sin(2 * np.pi * base_freq * 3 * t) +
                0.05 * np.sin(2 * np.pi * base_freq * 4 * t))
    
    def _generate_weak_beat(self, t: np.ndarray, base_freq: float) -> np.ndarray:
        """å¼±æ‹"""
        return (0.4 * np.sin(2 * np.pi * base_freq * t) +
                0.3 * np.sin(2 * np.pi * base_freq * 2 * t) +
                0.2 * np.sin(2 * np.pi * base_freq * 3 * t))


# ============================================================================
# æ•ˆæœéŸ³ç”Ÿæˆå™¨
# ============================================================================

class EffectGenerator(AudioGenerator):
    """æ•ˆæœéŸ³ç”Ÿæˆå™¨"""
    
    EFFECT_PRESETS = {
        EffectType.CORRECT: {
            'duration': 0.35,
            'frequencies': [(523.25, 0.4), (659.25, 0.3), (783.99, 0.25), (1046.50, 0.1)],
            'decay_rate': 4.0,
            'attack_rate': 50.0,
            'normalize_level': 0.8
        },
        EffectType.WRONG: {
            'duration': 0.25,
            'frequencies': [(220, 0.5), (185, 0.3), (110, 0.1)],
            'decay_rate': 6.0,
            'attack_rate': 100.0,
            'lowpass': 1500,
            'normalize_level': 0.7
        },
        EffectType.COMPLETE: {
            'duration': 1.0,
            'arpeggio': [(523.25, 0.0, 0.5), (659.25, 0.15, 0.45), 
                         (783.99, 0.30, 0.4), (1046.50, 0.45, 0.55)],
            'normalize_level': 0.8
        },
        EffectType.LEVEL_UP: {
            'duration': 1.2,
            'chord_frequencies': [(523.25, 0.4), (659.25, 0.3), (783.99, 0.25), 
                                  (987.77, 0.2), (1046.50, 0.15)],
            'normalize_level': 0.85
        }
    }
    
    def generate(self, effect_type: EffectType) -> Tuple[np.ndarray, int]:
        """ç”Ÿæˆæ•ˆæœéŸ³"""
        preset = self.EFFECT_PRESETS[effect_type]
        
        if effect_type == EffectType.CORRECT:
            audio = self._generate_correct(preset)
        elif effect_type == EffectType.WRONG:
            audio = self._generate_wrong(preset)
        elif effect_type == EffectType.COMPLETE:
            audio = self._generate_complete(preset)
        elif effect_type == EffectType.LEVEL_UP:
            audio = self._generate_levelup(preset)
        else:
            raise ValueError(f"Unknown effect type: {effect_type}")
        
        return audio, self.config.sample_rate
    
    def _generate_correct(self, preset: dict) -> np.ndarray:
        """æ­£ç¡®éŸ³æ•ˆ"""
        t = self._create_time_array(preset['duration'])
        audio = sum(amp * np.sin(2 * np.pi * freq * t) 
                    for freq, amp in preset['frequencies'])
        envelope = np.exp(-preset['decay_rate'] * t) * (1 - np.exp(-preset['attack_rate'] * t))
        audio *= envelope
        audio = self.processor.apply_fade(audio, 0, int(0.02 * self.config.sample_rate))
        audio = self.processor.normalize(audio, preset['normalize_level'])
        return self.processor.to_int16(audio)
    
    def _generate_wrong(self, preset: dict) -> np.ndarray:
        """é”™è¯¯éŸ³æ•ˆ"""
        t = self._create_time_array(preset['duration'])
        audio = sum(amp * np.sin(2 * np.pi * freq * t) 
                    for freq, amp in preset['frequencies'])
        envelope = np.exp(-preset['decay_rate'] * t) * (1 - np.exp(-preset['attack_rate'] * t))
        audio *= envelope
        audio = self.processor.lowpass_filter(audio, preset['lowpass'], self.config.sample_rate)
        audio = self.processor.apply_fade(audio, 0, int(0.02 * self.config.sample_rate))
        audio = self.processor.normalize(audio, preset['normalize_level'])
        return self.processor.to_int16(audio)
    
    def _generate_complete(self, preset: dict) -> np.ndarray:
        """å®ŒæˆéŸ³æ•ˆï¼ˆç¶éŸ³ï¼‰"""
        num_samples = int(self.config.sample_rate * preset['duration'])
        audio = np.zeros(num_samples)
        
        for freq, start_time, note_duration in preset['arpeggio']:
            start_sample = int(start_time * self.config.sample_rate)
            note_samples = int(note_duration * self.config.sample_rate)
            end_sample = min(start_sample + note_samples, num_samples)
            actual_samples = end_sample - start_sample
            
            t = np.linspace(0, note_duration, actual_samples)
            note = (0.6 * np.sin(2 * np.pi * freq * t) +
                    0.25 * np.sin(2 * np.pi * freq * 2 * t) +
                    0.1 * np.sin(2 * np.pi * freq * 3 * t))
            envelope = np.exp(-3 * t) * (1 - np.exp(-50 * t))
            audio[start_sample:end_sample] += note * envelope
        
        audio = self.processor.apply_fade(audio, 0, int(0.1 * self.config.sample_rate))
        audio = self.processor.normalize(audio, preset['normalize_level'])
        return self.processor.to_int16(audio)
    
    def _generate_levelup(self, preset: dict) -> np.ndarray:
        """å‡çº§éŸ³æ•ˆ"""
        num_samples = int(self.config.sample_rate * preset['duration'])
        t = np.linspace(0, preset['duration'], num_samples)
        audio = np.zeros(num_samples)
        
        # é¢‘ç‡æ»‘åŠ¨
        rise_duration = 0.3
        rise_samples = int(rise_duration * self.config.sample_rate)
        rise_t = np.linspace(0, rise_duration, rise_samples)
        freq_sweep = 400 + 400 * (rise_t / rise_duration) ** 0.5
        rise_audio = np.sin(2 * np.pi * np.cumsum(freq_sweep) / self.config.sample_rate)
        rise_envelope = np.linspace(0.3, 0.8, rise_samples)
        audio[:rise_samples] = rise_audio * rise_envelope
        
        # å’Œå¼¦
        chord_start = int(0.25 * self.config.sample_rate)
        chord_samples = num_samples - chord_start
        chord_t = np.linspace(0, preset['duration'] - 0.25, chord_samples)
        chord = sum(amp * np.sin(2 * np.pi * freq * chord_t) 
                    for freq, amp in preset['chord_frequencies'])
        chord_envelope = np.exp(-2 * chord_t) * (1 - np.exp(-30 * chord_t))
        audio[chord_start:] += chord * chord_envelope
        
        audio = self.processor.apply_fade(audio, 0, int(0.15 * self.config.sample_rate))
        audio = self.processor.normalize(audio, preset['normalize_level'])
        return self.processor.to_int16(audio)


# ============================================================================
# éŸ³é¢‘å¯¼å‡ºå™¨
# ============================================================================

class AudioExporter:
    """éŸ³é¢‘å¯¼å‡ºå™¨"""
    
    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        self.has_ffmpeg = self._check_ffmpeg()
    
    def _check_ffmpeg(self) -> bool:
        """æ£€æŸ¥ffmpegæ˜¯å¦å¯ç”¨"""
        try:
            subprocess.run(['ffmpeg', '-version'], capture_output=True, check=True)
            return True
        except (subprocess.CalledProcessError, FileNotFoundError):
            return False
    
    def export(self, audio: np.ndarray, sample_rate: int, filename: str, prefer_mp3: bool = True) -> Path:
        """å¯¼å‡ºéŸ³é¢‘æ–‡ä»¶"""
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        wav_path = self.output_dir / f"{filename}.wav"
        mp3_path = self.output_dir / f"{filename}.mp3"
        
        wavfile.write(str(wav_path), sample_rate, audio)
        
        if prefer_mp3 and self.has_ffmpeg:
            if self._convert_to_mp3(wav_path, mp3_path):
                wav_path.unlink()
                return mp3_path
        
        return wav_path
    
    def _convert_to_mp3(self, wav_path: Path, mp3_path: Path) -> bool:
        """è½¬æ¢ä¸ºMP3æ ¼å¼"""
        try:
            subprocess.run([
                'ffmpeg', '-i', str(wav_path),
                '-codec:a', 'libmp3lame',
                '-b:a', '192k',
                str(mp3_path), '-y'
            ], check=True, capture_output=True)
            return True
        except subprocess.CalledProcessError:
            return False


# ============================================================================
# ä¸»æµæ°´çº¿
# ============================================================================

class AudioGenerationPipeline:
    """éŸ³é¢‘ç”Ÿæˆæµæ°´çº¿"""
    
    def __init__(self, base_dir: Path, parallel: bool = False, analyze: bool = False):
        self.base_dir = base_dir
        self.assets_dir = base_dir / 'assets' / 'audio'
        self.config = AudioConfig()
        self.parallel = parallel
        self.analyze = analyze
        
        # è´¨é‡ç»Ÿè®¡
        self.quality_stats = {
            'total': 0,
            'accurate': 0,
            'high_snr': 0,
            'issues': []
        }
    
    def run(self):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        self._print_header()
        
        if self.analyze:
            print("ğŸ“Š åˆ†ææ¨¡å¼å·²å¯ç”¨ï¼Œå°†ç”Ÿæˆé¢‘è°±åˆ†æå›¾è¡¨\n")
        
        exporter = AudioExporter(self.assets_dir)
        if exporter.has_ffmpeg:
            print("âœ“ æ£€æµ‹åˆ° ffmpegï¼Œå°†ç”Ÿæˆé«˜è´¨é‡ MP3 æ ¼å¼")
        else:
            print("âš   æœªæ£€æµ‹åˆ° ffmpegï¼Œå°†ç”Ÿæˆ WAV æ ¼å¼")
            print("   æç¤ºï¼šbrew install ffmpeg (macOS) æˆ– apt install ffmpeg (Linux)\n")
        
        # ç”ŸæˆéŸ³é¢‘
        if self.parallel:
            print("ğŸš€ ä½¿ç”¨å¹¶è¡Œæ¨¡å¼åŠ é€Ÿç”Ÿæˆ...\n")
            self._generate_piano_notes_parallel(exporter)
        else:
            self._generate_piano_notes(exporter)
        
        self._generate_metronome_clicks(exporter)
        self._generate_effects(exporter)
        
        # æ˜¾ç¤ºè´¨é‡æŠ¥å‘Š
        self._print_quality_report()
        self._print_footer()
    
    def _generate_piano_notes(self, exporter: AudioExporter):
        """ä¸²è¡Œç”Ÿæˆé’¢ç´éŸ³ç¬¦ï¼ˆå¸¦è´¨é‡éªŒè¯ï¼‰"""
        print("1. ç”Ÿæˆé’¢ç´éŸ³ç¬¦ (å¢å¼ºéŸ³è‰² + è´¨é‡éªŒè¯)...")
        
        piano_dir = self.assets_dir / 'piano'
        self._clean_directory(piano_dir)
        piano_exporter = AudioExporter(piano_dir)
        
        generator = EnhancedPianoGenerator(self.config, EnhancedPianoConfig())
        analyzer = AudioQualityAnalyzer()
        
        # ç”¨äºåˆ†æçš„æ ·æœ¬éŸ³ç¬¦
        sample_notes = SAMPLE_NOTES_FOR_ANALYSIS if self.analyze else []
        
        for midi in MIDI_RANGE:
            audio, sr = generator.generate(midi)
            
            # è´¨é‡åˆ†æ
            spectrum_result = analyzer.analyze_spectrum(audio, sr, midi)
            snr = analyzer.calculate_snr(audio, sr)
            
            self.quality_stats['total'] += 1
            if spectrum_result['is_accurate']:
                self.quality_stats['accurate'] += 1
            if snr > SNR_THRESHOLD_DB:
                self.quality_stats['high_snr'] += 1
            
            # åªå°†éŸ³é«˜ä¸å‡†ç¡®ä½œä¸ºé—®é¢˜ï¼ŒSNRä½œä¸ºå‚è€ƒä¿¡æ¯
            if not spectrum_result['is_accurate']:
                self.quality_stats['issues'].append(
                    f"MIDI {midi}: éŸ³é«˜è¯¯å·®={spectrum_result['error_cents']:.1f}Â¢ (SNR={snr:.1f}dB)"
                )
            
            # å¯¼å‡º
            output_path = piano_exporter.export(audio, sr, f'note_{midi}')
            
            # å¯è§†åŒ–åˆ†æï¼ˆä»…æ ·æœ¬ï¼‰
            if self.analyze and midi in sample_notes:
                viz_dir = self.assets_dir / 'analysis'
                viz_dir.mkdir(exist_ok=True)
                output_viz_path = viz_dir / f'analysis_midi_{midi}.png'
                AudioVisualizer.plot_analysis(audio, sr, midi, output_viz_path)
                print(f"    ğŸ“Š åˆ†æå›¾è¡¨: {output_viz_path.name}")
            
            # ç®€æ´è¾“å‡ºï¼ˆåªæ ¹æ®éŸ³é«˜å‡†ç¡®ç‡åˆ¤æ–­çŠ¶æ€ï¼‰
            status = 'âœ“' if spectrum_result['is_accurate'] else 'âš '
            print(f"  {status} {output_path.name} | è¯¯å·®: {spectrum_result['error_cents']:.1f}Â¢ | SNR: {snr:.0f}dB")
        
        print(f"  å®Œæˆï¼ç”Ÿæˆäº† {len(MIDI_RANGE)} ä¸ªé’¢ç´éŸ³ç¬¦\n")
    
    def _generate_piano_notes_parallel(self, exporter: AudioExporter):
        """å¹¶è¡Œç”Ÿæˆé’¢ç´éŸ³ç¬¦"""
        print(f"1. ç”Ÿæˆé’¢ç´éŸ³ç¬¦ (å¹¶è¡Œæ¨¡å¼ï¼Œ{cpu_count()} è¿›ç¨‹)...")
        
        piano_dir = self.assets_dir / 'piano'
        self._clean_directory(piano_dir)
        
        midi_notes = list(MIDI_RANGE)
        
        with Pool(cpu_count()) as pool:
            generate_func = partial(self._generate_single_note, piano_dir)
            results = pool.map(generate_func, midi_notes)
        
        # ç»Ÿè®¡ç»“æœ
        for result in results:
            if result:
                self.quality_stats['total'] += 1
                if result['accurate']:
                    self.quality_stats['accurate'] += 1
                if result['high_snr']:
                    self.quality_stats['high_snr'] += 1
                if result['issue']:
                    self.quality_stats['issues'].append(result['issue'])
        
        # å¦‚æœå¯ç”¨åˆ†æï¼Œç”Ÿæˆåˆ†æå›¾è¡¨
        if self.analyze:
            print("\n  ç”Ÿæˆåˆ†æå›¾è¡¨...")
            generator = EnhancedPianoGenerator(self.config, EnhancedPianoConfig())
            viz_dir = self.assets_dir / 'analysis'
            viz_dir.mkdir(exist_ok=True)
            
            for midi in SAMPLE_NOTES_FOR_ANALYSIS:
                audio, sr = generator.generate(midi)
                output_path = viz_dir / f'analysis_midi_{midi}.png'
                AudioVisualizer.plot_analysis(audio, sr, midi, output_path)
                print(f"  âœ“ ç”Ÿæˆåˆ†æå›¾è¡¨: {output_path.name}")
        
        print(f"  å®Œæˆï¼ç”Ÿæˆäº† {len([r for r in results if r])} ä¸ªéŸ³ç¬¦\n")
    
    def _generate_single_note(self, output_dir: Path, midi: int) -> Optional[Dict]:
        """ç”Ÿæˆå•ä¸ªéŸ³ç¬¦ï¼ˆç”¨äºå¹¶è¡Œï¼‰"""
        try:
            config = AudioConfig()
            generator = EnhancedPianoGenerator(config, EnhancedPianoConfig())
            exporter = AudioExporter(output_dir)
            analyzer = AudioQualityAnalyzer()
            
            audio, sr = generator.generate(midi)
            
            spectrum_result = analyzer.analyze_spectrum(audio, sr, midi)
            snr = analyzer.calculate_snr(audio, sr)
            
            exporter.export(audio, sr, f'note_{midi}')
            
            return {
                'midi': midi,
                'accurate': spectrum_result['is_accurate'],
                'high_snr': snr > SNR_THRESHOLD_DB,
                'issue': None if spectrum_result['is_accurate'] 
                         else f"MIDI {midi}: éŸ³é«˜è¯¯å·®={spectrum_result['error_cents']:.1f}Â¢ (SNR={snr:.1f}dB)"
            }
        except Exception as e:
            return {'midi': midi, 'accurate': False, 'high_snr': False, 
                    'issue': f"MIDI {midi}: ç”Ÿæˆå¤±è´¥ - {e}"}
    
    def _generate_metronome_clicks(self, exporter: AudioExporter):
        """ç”ŸæˆèŠ‚æ‹å™¨éŸ³æ•ˆ"""
        print("2. ç”ŸæˆèŠ‚æ‹å™¨éŸ³æ•ˆ...")
        
        metronome_dir = self.assets_dir / 'metronome'
        self._clean_directory(metronome_dir)
        metronome_exporter = AudioExporter(metronome_dir)
        
        generator = MetronomeGenerator(self.config, MetronomeConfig())
        
        audio, sr = generator.generate(is_strong=True)
        output_path = metronome_exporter.export(audio, sr, 'click_strong')
        print(f"  âœ“ {output_path.name}")
        
        audio, sr = generator.generate(is_strong=False)
        output_path = metronome_exporter.export(audio, sr, 'click_weak')
        print(f"  âœ“ {output_path.name}")
        
        print("  å®Œæˆï¼\n")
    
    def _generate_effects(self, exporter: AudioExporter):
        """ç”Ÿæˆæ•ˆæœéŸ³"""
        print("3. ç”Ÿæˆæ•ˆæœéŸ³...")
        
        effects_dir = self.assets_dir / 'effects'
        self._clean_directory(effects_dir)
        effects_exporter = AudioExporter(effects_dir)
        
        generator = EffectGenerator(self.config)
        
        effect_names = {
            EffectType.CORRECT: 'å›ç­”æ­£ç¡®',
            EffectType.WRONG: 'å›ç­”é”™è¯¯',
            EffectType.COMPLETE: 'è®­ç»ƒå®Œæˆ',
            EffectType.LEVEL_UP: 'ç­‰çº§æå‡'
        }
        
        for effect_type in EffectType:
            audio, sr = generator.generate(effect_type)
            output_path = effects_exporter.export(audio, sr, effect_type.value)
            print(f"  âœ“ {output_path.name} ({effect_names[effect_type]})")
        
        print("  å®Œæˆï¼\n")
    
    def _clean_directory(self, directory: Path):
        """æ¸…ç†ç›®å½•"""
        if directory.exists():
            for f in directory.iterdir():
                if f.suffix in ['.mp3', '.wav']:
                    f.unlink()
        directory.mkdir(parents=True, exist_ok=True)
    
    def _print_header(self):
        """æ‰“å°æ ‡é¢˜"""
        print("=" * 70)
        print(" ğŸµ éŸ³é¢‘æ–‡ä»¶ç”Ÿæˆå™¨ v4.0 (å¢å¼ºä¼˜åŒ–ç‰ˆ)")
        print("=" * 70)
    
    def _print_quality_report(self):
        """æ‰“å°è´¨é‡æŠ¥å‘Š"""
        if self.quality_stats['total'] == 0:
            return
        
        print("\n" + "=" * 70)
        print(" ğŸ“Š è´¨é‡æŠ¥å‘Š")
        print("=" * 70)
        
        accuracy_rate = self.quality_stats['accurate'] / self.quality_stats['total'] * 100
        snr_rate = self.quality_stats['high_snr'] / self.quality_stats['total'] * 100
        
        print(f"  æ€»è®¡ï¼š{self.quality_stats['total']} ä¸ªéŸ³ç¬¦")
        print(f"  éŸ³é«˜å‡†ç¡®ç‡ï¼š{accuracy_rate:.1f}% ({self.quality_stats['accurate']}/{self.quality_stats['total']})")
        print(f"  é«˜ä¿¡å™ªæ¯”ç‡ï¼š{snr_rate:.1f}% ({self.quality_stats['high_snr']}/{self.quality_stats['total']}) [å‚è€ƒä¿¡æ¯]")
        
        if self.quality_stats['issues']:
            print(f"\n  âš ï¸  å‘ç° {len(self.quality_stats['issues'])} ä¸ªéŸ³é«˜é—®é¢˜ï¼š")
            for issue in self.quality_stats['issues'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"     - {issue}")
            if len(self.quality_stats['issues']) > 5:
                print(f"     ... è¿˜æœ‰ {len(self.quality_stats['issues']) - 5} ä¸ªé—®é¢˜")
        else:
            print("\n  âœ… æ‰€æœ‰éŸ³é¢‘éŸ³é«˜å‡†ç¡®ï¼Œè´¨é‡ä¼˜ç§€ï¼")
    
    def _print_footer(self):
        """æ‰“å°ç»“å°¾"""
        print("\n" + "=" * 70)
        print(" âœ… æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼")
        print("=" * 70)
        print("\nç‰¹æ€§ï¼š")
        print("  â€¢ å¢å¼ºéŸ³è‰²ï¼šéŸ³æ¿å…±é¸£ + ç´å¼¦è€¦åˆ + ä¸°å¯Œæ³›éŸ³")
        print("  â€¢ è´¨é‡éªŒè¯ï¼šé¢‘è°±åˆ†æ + SNRæ£€æµ‹ + THDè®¡ç®—")
        print("  â€¢ åŠ¨æ€ä¼˜åŒ–ï¼šæ ¹æ®éŸ³é«˜è°ƒæ•´åŒ…ç»œå’Œæ»¤æ³¢")
        if self.parallel:
            print(f"  â€¢ å¹¶è¡ŒåŠ é€Ÿï¼šä½¿ç”¨ {cpu_count()} ä¸ªè¿›ç¨‹")
        if self.analyze:
            print("  â€¢ å¯è§†åŒ–åˆ†æï¼šç”Ÿæˆé¢‘è°±å›¾å’Œè´¨é‡æŠ¥å‘Š")
        print()


# ============================================================================
# ä¸»ç¨‹åºå…¥å£
# ============================================================================

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='éŸ³é¢‘æ–‡ä»¶ç”Ÿæˆå™¨ v4.0',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ï¼š
  python3 scripts/generate_audio.py                    # åŸºç¡€ç”Ÿæˆ
  python3 scripts/generate_audio.py --parallel         # å¹¶è¡ŒåŠ é€Ÿ
  python3 scripts/generate_audio.py --analyze          # ç”Ÿæˆåˆ†æå›¾è¡¨
  python3 scripts/generate_audio.py --parallel --analyze  # å…¨åŠŸèƒ½
        """
    )
    
    parser.add_argument(
        '--parallel',
        action='store_true',
        help='ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œç”Ÿæˆï¼ˆåŠ é€Ÿ3-4å€ï¼‰'
    )
    
    parser.add_argument(
        '--analyze',
        action='store_true',
        help='ç”Ÿæˆæ ·æœ¬éŸ³ç¬¦çš„é¢‘è°±åˆ†æå›¾è¡¨'
    )
    
    args = parser.parse_args()
    
    try:
        script_dir = Path(__file__).parent
        base_dir = script_dir.parent
        
        # è°ƒè¯•è¾“å‡º
        if args.analyze:
            print("[DEBUG] analyze å‚æ•°å·²å¯ç”¨")
            print(f"[DEBUG] base_dir = {base_dir}")
            print(f"[DEBUG] assets_dir = {base_dir / 'assets' / 'audio'}\n")
        
        pipeline = AudioGenerationPipeline(
            base_dir, 
            parallel=args.parallel,
            analyze=args.analyze
        )
        pipeline.run()
        
        return 0
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"\n[ERROR] é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


def demo_smart_mixing():
    """
    æ¼”ç¤ºæ™ºèƒ½æ··éŸ³æŠ€æœ¯
    
    å±•ç¤ºä¸ºä»€ä¹ˆé’¢ç´è½¯ä»¶å¯ä»¥åŒæ—¶æ’­æ”¾å¤šä¸ªéŸ³ç¬¦è€Œä¸ä¼šå¤±çœŸ
    """
    print("\n" + "=" * 70)
    print(" ğŸ¹ æ™ºèƒ½æ··éŸ³æŠ€æœ¯æ¼”ç¤º")
    print("=" * 70)
    print("\né—®é¢˜ï¼šä¸ºä»€ä¹ˆé¢„å½•éŸ³é¢‘å åŠ æ•ˆæœå·®ï¼Œä½†é’¢ç´è½¯ä»¶å¯ä»¥åŒæ—¶æ’­æ”¾å¤šéŸ³ï¼Ÿ")
    print("\nåŸå› åˆ†æï¼š")
    print("  1. ç›¸ä½å¯¹é½ï¼šé¢„å½•éŸ³é¢‘ç›¸ä½å›ºå®šï¼Œå åŠ æ—¶å¯èƒ½å¢å¼ºæˆ–æŠµæ¶ˆ")
    print("  2. éŸ³é‡çº¿æ€§å¢é•¿ï¼šç®€å•ç›¸åŠ å¯¼è‡´éŸ³é‡ = N Ã— å•éŸ³ï¼Œå®¹æ˜“å‰Šæ³¢")
    print("  3. æ²¡æœ‰åŠ¨æ€å¤„ç†ï¼šç¼ºå°‘å‹ç¼©å’Œè½¯å‰Šæ³¢ä¿æŠ¤")
    print("\nè§£å†³æ–¹æ¡ˆï¼ˆé’¢ç´è½¯ä»¶ä½¿ç”¨çš„æŠ€æœ¯ï¼‰ï¼š")
    print("  âœ“ RMSå½’ä¸€åŒ–ï¼šéŸ³é‡æŒ‰ âˆšN å¢é•¿ï¼Œè€Œä¸æ˜¯ N")
    print("  âœ“ è½¯å‰Šæ³¢ï¼šé˜²æ­¢ç¡¬å‰Šæ³¢å¤±çœŸ")
    print("  âœ“ ç›¸ä½éšæœºåŒ–ï¼šé¿å…ç›¸ä½å¯¹é½é—®é¢˜")
    print("  âœ“ åŠ¨æ€å‹ç¼©ï¼šæ§åˆ¶å³°å€¼")
    print("\nç”Ÿæˆå¯¹æ¯”ç¤ºä¾‹...")
    
    config = AudioConfig()
    generator = EnhancedPianoGenerator(config, EnhancedPianoConfig())
    processor = AudioProcessor()
    
    # ç”ŸæˆCå¤§è°ƒå’Œå¼¦çš„ä¸‰ä¸ªéŸ³ç¬¦
    midi_notes = [60, 64, 67]  # C, E, G
    note_audios = []
    
    for midi in midi_notes:
        audio, sr = generator.generate(midi)
        # è½¬æ¢ä¸ºæµ®ç‚¹æ•°ç”¨äºæ··éŸ³
        audio_float = audio.astype(np.float64) / 32767.0
        note_audios.append(audio_float)
    
    # æ–¹æ³•1ï¼šç®€å•ç›¸åŠ ï¼ˆæ•ˆæœå·®ï¼‰
    simple_mix = processor.mix(note_audios, use_smart_mixing=False)
    simple_mix = processor.normalize(simple_mix, 0.9)
    simple_mix_int16 = processor.to_int16(simple_mix)
    
    # æ–¹æ³•2ï¼šæ™ºèƒ½æ··éŸ³ï¼ˆæ•ˆæœå¥½ï¼‰
    smart_mix = processor.mix(note_audios, use_smart_mixing=True)
    smart_mix = processor.normalize(smart_mix, 0.9)
    smart_mix_int16 = processor.to_int16(smart_mix)
    
    # ä¿å­˜å¯¹æ¯”
    output_dir = Path('assets/audio/demo')
    output_dir.mkdir(parents=True, exist_ok=True)
    
    wavfile.write(str(output_dir / 'chord_simple_mix.wav'), config.sample_rate, simple_mix_int16)
    wavfile.write(str(output_dir / 'chord_smart_mix.wav'), config.sample_rate, smart_mix_int16)
    
    # åˆ†æå³°å€¼
    simple_peak = np.max(np.abs(simple_mix))
    smart_peak = np.max(np.abs(smart_mix))
    
    print(f"\nå¯¹æ¯”ç»“æœï¼š")
    print(f"  ç®€å•æ··éŸ³å³°å€¼: {simple_peak:.3f} (å¯èƒ½å‰Šæ³¢)")
    print(f"  æ™ºèƒ½æ··éŸ³å³°å€¼: {smart_peak:.3f} (å®‰å…¨èŒƒå›´)")
    print(f"\næ–‡ä»¶å·²ä¿å­˜ï¼š")
    print(f"  - {output_dir / 'chord_simple_mix.wav'} (ç®€å•æ··éŸ³)")
    print(f"  - {output_dir / 'chord_smart_mix.wav'} (æ™ºèƒ½æ··éŸ³)")
    print("\n" + "=" * 70)


if __name__ == '__main__':
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == '--demo-mixing':
        demo_smart_mixing()
    else:
        exit(main())
